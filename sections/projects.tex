\section{Publications/Open Source Contributions}
\vspace{-8pt}
\resumeSubheadingProject
{Integrated Energy Optimizer into Megatron-LM [\href{https://github.com/DdIiVvYyAaMm/zeus-profiler/tree/zeus-energy-profiler}{GitHub} | \href{https://symbioticlab.org/}{Symbiotic Lab}]}{Jan 2025 -- Apr 2025}
\resumeItemListStart
    \resumeItemNoTitle{Integrated an Energy Optimizer into \href{https://github.com/ml-energy/zeus}{Zeus} as the Pipeline Frequency Optimizer (PFO), implementing dynamic GPU frequency scheduling via runtime profiling to significantly reduce energy usage without compromising training throughput}
    \resumeItemNoTitle{Developed robust client-server POST endpoints for per-instruction energy and timing profiling, designed energy polling loop, and monkey-patched Zeus into Megatron-LM for seamless, fork-free compatibility across future versions}
    \resumeItemNoTitle{Successfully demonstrated integration effectiveness through a comprehensive README that enables easy replication of GPT-based training on custom datasets using Megatron-LM, ensuring practical usability and verifiable results}
\resumeItemListEnd

\resumeSubheadingProject
  {MoDM: Serving T2I Generation via Mixture-of-Diffusion Models [\href{https://arxiv.org/abs/2503.11972}{Paper}]}{Oct 2024 -- Mar 2025}
\resumeItemListStart
    \resumeItemNoTitle{Designed and implemented a novel serving system for diffusion models, dynamically balancing inference latency and image quality using a mixture of models from the Stable Diffusion and SANA families}
    \resumeItemNoTitle{Achieved a 2.5$\times$ reduction in serving time and 46.7\% lower energy consumption through adaptive caching and dynamic GPU resource allocation benchmarked on DiffusionDB and MJHQ-30k datasets; paper submitted to ASPLOSâ€™26}
\resumeItemListEnd

\resumeSubheadingProject
{Optimizing GEMM Operations via LLVM-based Loop Tiling [\href{https://github.com/DdIiVvYyAaMm/Computation-Communication-Fusion/tree/master}{GitHub}]}{Aug 2024 -- Nov 2024}
\resumeItemListStart
    \resumeItemNoTitle{Implemented LLVM optimization pass for loop tiling, enhancing GEMM operation performance by fusing NCCL communication and CUDA computation kernels achieving 94\% L1 cache hit rate over 27\% for manual CUDA kernels}
    \resumeItemNoTitle{Improved GPU efficiency and parallelism by dynamically selecting tile sizes and modifying Intermediate Representation (IR),
    reducing memory latency by 23\% and execution time for GEMM in high-performance AI/ML workloads}
\resumeItemListEnd